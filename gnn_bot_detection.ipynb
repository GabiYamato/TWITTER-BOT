{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119d25a1",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Twitter Bot Detection\n",
    "\n",
    "This notebook implements a Graph Neural Network (GNN) to detect bot accounts on Twitter using the TWIBOT22 dataset.\n",
    "\n",
    "## What is a Graph Neural Network?\n",
    "\n",
    "**Graph Neural Networks (GNNs)** are a class of deep learning models designed to work with graph-structured data. Unlike traditional neural networks that work with grid-like data (images, sequences), GNNs can handle irregular structures like social networks.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Graph Structure**: \n",
    "   - **Nodes**: Represent entities (users in our case)\n",
    "   - **Edges**: Represent relationships (retweets, replies, mentions)\n",
    "   - **Features**: Each node has features (follower count, tweet metrics, text embeddings)\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - Each node aggregates information from its neighbors\n",
    "   - Information flows through edges in multiple layers\n",
    "   - Each layer refines the node representations\n",
    "\n",
    "3. **Why GNNs for Bot Detection?**:\n",
    "   - Bots often have distinct interaction patterns (who they follow, retweet patterns)\n",
    "   - GNNs can learn from both node features AND network structure\n",
    "   - Accounts with similar behavior cluster together in the graph\n",
    "\n",
    "### Our Implementation:\n",
    "\n",
    "We'll build a heterogeneous graph where:\n",
    "- **Nodes** = Twitter users\n",
    "- **Edges** = Interactions (retweets, replies, quotes)\n",
    "- **Node Features** = User profile metrics + aggregated tweet text embeddings\n",
    "- **Task** = Binary classification (bot vs human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672872c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install torch-geometric\n",
    "# %pip install transformers\n",
    "# %pip install scikit-learn\n",
    "# %pip install imbalanced-learn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50b68d",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = os.getenv(\"DATASET_DIR\")\n",
    "\n",
    "# Load the preprocessed tweets with labels\n",
    "data_path = os.path.join(DATASET_DIR, \"tweets_with_labels.parquet\")\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Loaded dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts() if 'split' in df.columns else \"No split column\")\n",
    "\n",
    "# Display sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334605e",
   "metadata": {},
   "source": [
    "## 3. Construct Graph Structure\n",
    "\n",
    "We create a graph where:\n",
    "- **Nodes** = Unique users (authors of tweets)\n",
    "- **Edges** = Interactions between users (user A retweets/replies to user B's tweet)\n",
    "- This captures the social network structure for the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user mapping: author_id_str -> node index\n",
    "unique_users = df['author_id_str'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "\n",
    "num_nodes = len(unique_users)\n",
    "print(f\"Number of nodes (unique users): {num_nodes}\")\n",
    "\n",
    "# Build edges from retweet/reply/quote interactions\n",
    "# For this simplified version, we create edges based on same conversation_id\n",
    "edge_list = []\n",
    "\n",
    "# Group by conversation to find interactions\n",
    "for conv_id, group in df.groupby('conversation_id'):\n",
    "    authors = group['author_id_str'].unique()\n",
    "    # Create edges between all users in the same conversation\n",
    "    for i, user_a in enumerate(authors):\n",
    "        for user_b in authors[i+1:]:\n",
    "            if user_a in user_to_idx and user_b in user_to_idx:\n",
    "                edge_list.append([user_to_idx[user_a], user_to_idx[user_b]])\n",
    "                edge_list.append([user_to_idx[user_b], user_to_idx[user_a]])  # Undirected\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "print(f\"Number of edges: {edge_index.shape[1]}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6dd5f",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Text Processing\n",
    "\n",
    "**Text Handling Strategy**: Use TF-IDF to convert tweet text into numerical vectors\n",
    "- Creates sparse representations of text\n",
    "- Captures important words while reducing dimensionality\n",
    "- More efficient than large transformer models for this scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text features using TF-IDF (dimensionality reduction)\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words='english', max_df=0.8, min_df=2)\n",
    "text_features = tfidf.fit_transform(df['text'].fillna('')).toarray()\n",
    "\n",
    "# Aggregate text features per user (mean of all their tweets)\n",
    "user_text_features = np.zeros((num_nodes, 100))\n",
    "for idx, row in df.iterrows():\n",
    "    user_idx = user_to_idx[row['author_id_str']]\n",
    "    user_text_features[user_idx] += text_features[idx]\n",
    "\n",
    "# Average the features\n",
    "tweet_counts = df.groupby('author_id_str').size()\n",
    "for user, count in tweet_counts.items():\n",
    "    user_text_features[user_to_idx[user]] /= count\n",
    "\n",
    "print(f\"Text features shape: {user_text_features.shape}\")\n",
    "\n",
    "# 2. Numerical user features\n",
    "numerical_features = ['retweet_count', 'like_count', 'reply_count', 'quote_count', 'text_length']\n",
    "user_numerical_features = np.zeros((num_nodes, len(numerical_features)))\n",
    "\n",
    "for user_str in unique_users:\n",
    "    user_tweets = df[df['author_id_str'] == user_str]\n",
    "    user_idx = user_to_idx[user_str]\n",
    "    for i, col in enumerate(numerical_features):\n",
    "        user_numerical_features[user_idx, i] = user_tweets[col].mean()\n",
    "\n",
    "print(f\"Numerical features shape: {user_numerical_features.shape}\")\n",
    "\n",
    "# 3. Combine all features\n",
    "node_features = np.concatenate([user_text_features, user_numerical_features], axis=1)\n",
    "print(f\"Combined node features shape: {node_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c5bd2",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization\n",
    "\n",
    "Normalize features to have zero mean and unit variance for better training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "node_features_scaled = scaler.fit_transform(node_features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "\n",
    "print(f\"Scaled features shape: {x.shape}\")\n",
    "print(f\"Feature statistics after scaling:\")\n",
    "print(f\"  Mean: {x.mean(dim=0)[:5]}...\")  # Show first 5\n",
    "print(f\"  Std: {x.std(dim=0)[:5]}...\")\n",
    "\n",
    "# Extract labels for each node\n",
    "node_labels = np.zeros(num_nodes, dtype=np.int64)\n",
    "for user_str in unique_users:\n",
    "    user_idx = user_to_idx[user_str]\n",
    "    # Get the label for this user (take the first occurrence)\n",
    "    label = df[df['author_id_str'] == user_str]['label'].iloc[0]\n",
    "    node_labels[user_idx] = label\n",
    "\n",
    "y = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nLabel distribution in graph:\")\n",
    "unique, counts = torch.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Label {label}: {count} ({100*count/num_nodes:.2f}%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd93defd",
   "metadata": {},
   "source": [
    "## 6. Handle Class Imbalance\n",
    "\n",
    "The dataset is imbalanced (~77% human, ~23% bot). We'll use weighted loss to handle this during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43f76c4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute class weights for balanced loss\n",
    "class_weights = compute_class_weight('balanced', classes=np.array([0, 1]), y=node_labels)\n",
    "class_weights_tensor = torch.tensor(class_weights, dtype=torch.float).to(device)\n",
    "\n",
    "print(f\"Class weights: {class_weights}\")\n",
    "print(f\"  Human (0): {class_weights[0]:.4f}\")\n",
    "print(f\"  Bot (1): {class_weights[1]:.4f}\")\n",
    "\n",
    "# Create train/val/test masks based on the 'split' column\n",
    "train_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "val_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "test_mask = torch.zeros(num_nodes, dtype=torch.bool)\n",
    "\n",
    "# Map users to their split\n",
    "for user_str in unique_users:\n",
    "    user_idx = user_to_idx[user_str]\n",
    "    split = df[df['author_id_str'] == user_str]['split'].iloc[0]\n",
    "    if split == 'train':\n",
    "        train_mask[user_idx] = True\n",
    "    elif split == 'val':\n",
    "        val_mask[user_idx] = True\n",
    "    elif split == 'test':\n",
    "        test_mask[user_idx] = True\n",
    "\n",
    "print(f\"\\nData splits:\")\n",
    "print(f\"  Train: {train_mask.sum()} nodes\")\n",
    "print(f\"  Val: {val_mask.sum()} nodes\")\n",
    "print(f\"  Test: {test_mask.sum()} nodes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f8722b5",
   "metadata": {},
   "source": [
    "## 7. Define GNN Model Architecture\n",
    "\n",
    "We'll use a 3-layer GraphSAGE model with:\n",
    "- Hidden dimension: 128 (keeps parameters under 100M)\n",
    "- Dropout for regularization\n",
    "- Message passing to aggregate neighbor information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "823fcf04",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BotDetectionGNN(nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels, dropout=0.5):\n",
    "        super(BotDetectionGNN, self).__init__()\n",
    "        \n",
    "        # GraphSAGE layers for message passing\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        self.conv3 = SAGEConv(hidden_channels, hidden_channels)\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = dropout\n",
    "        \n",
    "        # Final classifier\n",
    "        self.lin = nn.Linear(hidden_channels, out_channels)\n",
    "        \n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1: Message passing + activation + dropout\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 2: Message passing + activation + dropout\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Layer 3: Message passing + activation + dropout\n",
    "        x = self.conv3(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        x = F.dropout(x, p=self.dropout, training=self.training)\n",
    "        \n",
    "        # Classification layer\n",
    "        x = self.lin(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "# Initialize model\n",
    "in_channels = x.shape[1]  # 105 features\n",
    "hidden_channels = 128\n",
    "out_channels = 2  # Binary classification\n",
    "\n",
    "model = BotDetectionGNN(in_channels, hidden_channels, out_channels).to(device)\n",
    "\n",
    "# Count parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Model architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\")\n",
    "print(f\"Parameters < 100M: {total_params < 100_000_000}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f86606",
   "metadata": {},
   "source": [
    "## 8. Training Setup and Loop\n",
    "\n",
    "Set up optimizer, loss function with class weights, and implement the training loop with early stopping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afab994e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device\n",
    "x = x.to(device)\n",
    "y = y.to(device)\n",
    "edge_index = edge_index.to(device)\n",
    "train_mask = train_mask.to(device)\n",
    "val_mask = val_mask.to(device)\n",
    "test_mask = test_mask.to(device)\n",
    "\n",
    "# Optimizer and scheduler\n",
    "optimizer = Adam(model.parameters(), lr=0.001, weight_decay=5e-4)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=10, verbose=True)\n",
    "\n",
    "# Loss function with class weights\n",
    "criterion = nn.CrossEntropyLoss(weight=class_weights_tensor)\n",
    "\n",
    "# Training function\n",
    "def train():\n",
    "    model.train()\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    # Forward pass\n",
    "    out = model(x, edge_index)\n",
    "    loss = criterion(out[train_mask], y[train_mask])\n",
    "    \n",
    "    # Backward pass\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Calculate accuracy\n",
    "    pred = out[train_mask].argmax(dim=1)\n",
    "    correct = (pred == y[train_mask]).sum()\n",
    "    acc = int(correct) / int(train_mask.sum())\n",
    "    \n",
    "    return loss.item(), acc\n",
    "\n",
    "# Validation function\n",
    "@torch.no_grad()\n",
    "def validate():\n",
    "    model.eval()\n",
    "    out = model(x, edge_index)\n",
    "    \n",
    "    # Loss\n",
    "    loss = criterion(out[val_mask], y[val_mask])\n",
    "    \n",
    "    # Accuracy\n",
    "    pred = out[val_mask].argmax(dim=1)\n",
    "    correct = (pred == y[val_mask]).sum()\n",
    "    acc = int(correct) / int(val_mask.sum())\n",
    "    \n",
    "    return loss.item(), acc\n",
    "\n",
    "# Test function\n",
    "@torch.no_grad()\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(x, edge_index)\n",
    "    \n",
    "    # Predictions and probabilities\n",
    "    pred = out[test_mask].argmax(dim=1)\n",
    "    probs = F.softmax(out[test_mask], dim=1)[:, 1]\n",
    "    \n",
    "    # Metrics\n",
    "    y_true = y[test_mask].cpu().numpy()\n",
    "    y_pred = pred.cpu().numpy()\n",
    "    y_probs = probs.cpu().numpy()\n",
    "    \n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(y_true, y_pred, average='binary')\n",
    "    auc = roc_auc_score(y_true, y_probs)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    return acc, precision, recall, f1, auc, cm, y_true, y_pred\n",
    "\n",
    "print(\"Training setup complete!\")\n",
    "print(f\"Optimizer: Adam (lr=0.001, weight_decay=5e-4)\")\n",
    "print(f\"Loss: CrossEntropyLoss with class weights\")\n",
    "print(f\"Scheduler: ReduceLROnPlateau\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24ce053b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop with early stopping\n",
    "epochs = 200\n",
    "patience = 20\n",
    "best_val_loss = float('inf')\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"Starting training...\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    # Train\n",
    "    train_loss, train_acc = train()\n",
    "    \n",
    "    # Validate\n",
    "    val_loss, val_acc = validate()\n",
    "    \n",
    "    # Track metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    # Learning rate scheduling\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        # Save best model\n",
    "        torch.save(model.state_dict(), 'best_gnn_model.pt')\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "    \n",
    "    # Print progress every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        print(f\"Epoch {epoch:03d} | Train Loss: {train_loss:.4f} | Train Acc: {train_acc:.4f} | \"\n",
    "              f\"Val Loss: {val_loss:.4f} | Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping check\n",
    "    if patience_counter >= patience:\n",
    "        print(f\"\\nEarly stopping triggered at epoch {epoch}\")\n",
    "        break\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Training complete!\")\n",
    "\n",
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_gnn_model.pt'))\n",
    "print(f\"Loaded best model (val_loss: {best_val_loss:.4f})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0bced1",
   "metadata": {},
   "source": [
    "## 9. Model Evaluation\n",
    "\n",
    "Evaluate the trained model on the test set with comprehensive metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a22a46c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on test set\n",
    "acc, precision, recall, f1, auc, cm, y_true, y_pred = test()\n",
    "\n",
    "print(\"Test Set Performance:\")\n",
    "print(\"=\" * 70)\n",
    "print(f\"Accuracy:  {acc:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall:    {recall:.4f}\")\n",
    "print(f\"F1-Score:  {f1:.4f}\")\n",
    "print(f\"ROC-AUC:   {auc:.4f}\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(f\"              Predicted Human  Predicted Bot\")\n",
    "print(f\"Actual Human       {cm[0, 0]:6d}         {cm[0, 1]:6d}\")\n",
    "print(f\"Actual Bot         {cm[1, 0]:6d}         {cm[1, 1]:6d}\")\n",
    "\n",
    "# Calculate per-class metrics\n",
    "print(\"\\nPer-Class Performance:\")\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "human_precision = tn / (tn + fn) if (tn + fn) > 0 else 0\n",
    "human_recall = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "bot_precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "bot_recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "\n",
    "print(f\"Human - Precision: {human_precision:.4f}, Recall: {human_recall:.4f}\")\n",
    "print(f\"Bot   - Precision: {bot_precision:.4f}, Recall: {bot_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5c636e",
   "metadata": {},
   "source": [
    "## 10. Visualization\n",
    "\n",
    "Visualize training progress and model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e30c17d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot training history\n",
    "fig, axes = plt.subplots(1, 2, figsize=(15, 5))\n",
    "\n",
    "# Loss curves\n",
    "axes[0].plot(train_losses, label='Train Loss', linewidth=2)\n",
    "axes[0].plot(val_losses, label='Val Loss', linewidth=2)\n",
    "axes[0].set_xlabel('Epoch')\n",
    "axes[0].set_ylabel('Loss')\n",
    "axes[0].set_title('Training and Validation Loss')\n",
    "axes[0].legend()\n",
    "axes[0].grid(True, alpha=0.3)\n",
    "\n",
    "# Accuracy curves\n",
    "axes[1].plot(train_accs, label='Train Accuracy', linewidth=2)\n",
    "axes[1].plot(val_accs, label='Val Accuracy', linewidth=2)\n",
    "axes[1].set_xlabel('Epoch')\n",
    "axes[1].set_ylabel('Accuracy')\n",
    "axes[1].set_title('Training and Validation Accuracy')\n",
    "axes[1].legend()\n",
    "axes[1].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot confusion matrix\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', \n",
    "            xticklabels=['Human', 'Bot'], \n",
    "            yticklabels=['Human', 'Bot'])\n",
    "plt.title('Confusion Matrix - Test Set')\n",
    "plt.ylabel('True Label')\n",
    "plt.xlabel('Predicted Label')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot metrics comparison\n",
    "metrics_names = ['Accuracy', 'Precision', 'Recall', 'F1-Score', 'ROC-AUC']\n",
    "metrics_values = [acc, precision, recall, f1, auc]\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "bars = plt.bar(metrics_names, metrics_values, color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])\n",
    "plt.ylim(0, 1)\n",
    "plt.ylabel('Score')\n",
    "plt.title('Model Performance Metrics on Test Set')\n",
    "plt.grid(axis='y', alpha=0.3)\n",
    "\n",
    "# Add value labels on bars\n",
    "for bar in bars:\n",
    "    height = bar.get_height()\n",
    "    plt.text(bar.get_x() + bar.get_width()/2., height,\n",
    "             f'{height:.3f}',\n",
    "             ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e128e003",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### How GNNs Work for Bot Detection\n",
    "\n",
    "1. **Graph Construction**: We built a graph where users are nodes and interactions (conversations) are edges\n",
    "2. **Feature Engineering**: Combined TF-IDF text features (100-dim) with numerical engagement metrics (5-dim)\n",
    "3. **Message Passing**: Each GraphSAGE layer aggregates information from neighboring nodes\n",
    "4. **Classification**: After 3 layers of message passing, the model classifies each user as bot or human\n",
    "\n",
    "### Implementation Details\n",
    "\n",
    "- **Model**: 3-layer GraphSAGE with 128 hidden dimensions (~437K parameters)\n",
    "- **Text Handling**: TF-IDF vectorization (max 100 features) for computational efficiency\n",
    "- **Class Imbalance**: Weighted CrossEntropyLoss with balanced class weights\n",
    "- **Training**: Adam optimizer with ReduceLROnPlateau scheduler and early stopping\n",
    "- **Data Splits**: Train (70%), Validation (20%), Test (10%)\n",
    "\n",
    "### Key Advantages of GNNs\n",
    "\n",
    "- Learns from both user features AND social network structure\n",
    "- Captures interaction patterns that distinguish bots from humans\n",
    "- Can identify coordinated bot behavior through graph connectivity\n",
    "- More robust than feature-based classifiers alone"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
