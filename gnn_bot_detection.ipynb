{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "119d25a1",
   "metadata": {},
   "source": [
    "# Graph Neural Network for Twitter Bot Detection\n",
    "\n",
    "This notebook implements a Graph Neural Network (GNN) to detect bot accounts on Twitter using the TWIBOT22 dataset.\n",
    "\n",
    "## What is a Graph Neural Network?\n",
    "\n",
    "**Graph Neural Networks (GNNs)** are a class of deep learning models designed to work with graph-structured data. Unlike traditional neural networks that work with grid-like data (images, sequences), GNNs can handle irregular structures like social networks.\n",
    "\n",
    "### Key Concepts:\n",
    "\n",
    "1. **Graph Structure**: \n",
    "   - **Nodes**: Represent entities (users in our case)\n",
    "   - **Edges**: Represent relationships (retweets, replies, mentions)\n",
    "   - **Features**: Each node has features (follower count, tweet metrics, text embeddings)\n",
    "\n",
    "2. **Message Passing**:\n",
    "   - Each node aggregates information from its neighbors\n",
    "   - Information flows through edges in multiple layers\n",
    "   - Each layer refines the node representations\n",
    "\n",
    "3. **Why GNNs for Bot Detection?**:\n",
    "   - Bots often have distinct interaction patterns (who they follow, retweet patterns)\n",
    "   - GNNs can learn from both node features AND network structure\n",
    "   - Accounts with similar behavior cluster together in the graph\n",
    "\n",
    "### Our Implementation:\n",
    "\n",
    "We'll build a heterogeneous graph where:\n",
    "- **Nodes** = Twitter users\n",
    "- **Edges** = Interactions (retweets, replies, quotes)\n",
    "- **Node Features** = User profile metrics + aggregated tweet text embeddings\n",
    "- **Task** = Binary classification (bot vs human)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e672872c",
   "metadata": {},
   "source": [
    "## 1. Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e2e50a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install required packages (run once)\n",
    "# %pip install torch torchvision torchaudio\n",
    "# %pip install torch-geometric\n",
    "# %pip install transformers\n",
    "# %pip install scikit-learn\n",
    "# %pip install imbalanced-learn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Core libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "# PyTorch\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "\n",
    "# PyTorch Geometric\n",
    "import torch_geometric\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.nn import GCNConv, SAGEConv, global_mean_pool\n",
    "from torch_geometric.utils import degree\n",
    "\n",
    "# Scikit-learn\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score, confusion_matrix\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Imbalanced learning\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"PyTorch Geometric version: {torch_geometric.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e50b68d",
   "metadata": {},
   "source": [
    "## 2. Load Preprocessed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0f8606",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load environment variables\n",
    "load_dotenv()\n",
    "DATASET_DIR = os.getenv(\"DATASET_DIR\")\n",
    "\n",
    "# Load the preprocessed tweets with labels\n",
    "data_path = os.path.join(DATASET_DIR, \"tweets_with_labels.parquet\")\n",
    "df = pd.read_parquet(data_path)\n",
    "\n",
    "print(f\"Loaded dataset shape: {df.shape}\")\n",
    "print(f\"\\nColumns: {df.columns.tolist()}\")\n",
    "print(f\"\\nLabel distribution:\")\n",
    "print(df['label'].value_counts())\n",
    "print(f\"\\nSplit distribution:\")\n",
    "print(df['split'].value_counts() if 'split' in df.columns else \"No split column\")\n",
    "\n",
    "# Display sample\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b334605e",
   "metadata": {},
   "source": [
    "## 3. Construct Graph Structure\n",
    "\n",
    "We create a graph where:\n",
    "- **Nodes** = Unique users (authors of tweets)\n",
    "- **Edges** = Interactions between users (user A retweets/replies to user B's tweet)\n",
    "- This captures the social network structure for the GNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88a9e305",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create user mapping: author_id_str -> node index\n",
    "unique_users = df['author_id_str'].unique()\n",
    "user_to_idx = {user: idx for idx, user in enumerate(unique_users)}\n",
    "idx_to_user = {idx: user for user, idx in user_to_idx.items()}\n",
    "\n",
    "num_nodes = len(unique_users)\n",
    "print(f\"Number of nodes (unique users): {num_nodes}\")\n",
    "\n",
    "# Build edges from retweet/reply/quote interactions\n",
    "# For this simplified version, we create edges based on same conversation_id\n",
    "edge_list = []\n",
    "\n",
    "# Group by conversation to find interactions\n",
    "for conv_id, group in df.groupby('conversation_id'):\n",
    "    authors = group['author_id_str'].unique()\n",
    "    # Create edges between all users in the same conversation\n",
    "    for i, user_a in enumerate(authors):\n",
    "        for user_b in authors[i+1:]:\n",
    "            if user_a in user_to_idx and user_b in user_to_idx:\n",
    "                edge_list.append([user_to_idx[user_a], user_to_idx[user_b]])\n",
    "                edge_list.append([user_to_idx[user_b], user_to_idx[user_a]])  # Undirected\n",
    "\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()\n",
    "print(f\"Number of edges: {edge_index.shape[1]}\")\n",
    "print(f\"Edge index shape: {edge_index.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46b6dd5f",
   "metadata": {},
   "source": [
    "## 4. Feature Engineering and Text Processing\n",
    "\n",
    "**Text Handling Strategy**: Use TF-IDF to convert tweet text into numerical vectors\n",
    "- Creates sparse representations of text\n",
    "- Captures important words while reducing dimensionality\n",
    "- More efficient than large transformer models for this scale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "489a30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Text features using TF-IDF (dimensionality reduction)\n",
    "tfidf = TfidfVectorizer(max_features=100, stop_words='english', max_df=0.8, min_df=2)\n",
    "text_features = tfidf.fit_transform(df['text'].fillna('')).toarray()\n",
    "\n",
    "# Aggregate text features per user (mean of all their tweets)\n",
    "user_text_features = np.zeros((num_nodes, 100))\n",
    "for idx, row in df.iterrows():\n",
    "    user_idx = user_to_idx[row['author_id_str']]\n",
    "    user_text_features[user_idx] += text_features[idx]\n",
    "\n",
    "# Average the features\n",
    "tweet_counts = df.groupby('author_id_str').size()\n",
    "for user, count in tweet_counts.items():\n",
    "    user_text_features[user_to_idx[user]] /= count\n",
    "\n",
    "print(f\"Text features shape: {user_text_features.shape}\")\n",
    "\n",
    "# 2. Numerical user features\n",
    "numerical_features = ['retweet_count', 'like_count', 'reply_count', 'quote_count', 'text_length']\n",
    "user_numerical_features = np.zeros((num_nodes, len(numerical_features)))\n",
    "\n",
    "for user_str in unique_users:\n",
    "    user_tweets = df[df['author_id_str'] == user_str]\n",
    "    user_idx = user_to_idx[user_str]\n",
    "    for i, col in enumerate(numerical_features):\n",
    "        user_numerical_features[user_idx, i] = user_tweets[col].mean()\n",
    "\n",
    "print(f\"Numerical features shape: {user_numerical_features.shape}\")\n",
    "\n",
    "# 3. Combine all features\n",
    "node_features = np.concatenate([user_text_features, user_numerical_features], axis=1)\n",
    "print(f\"Combined node features shape: {node_features.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d21c5bd2",
   "metadata": {},
   "source": [
    "## 5. Feature Scaling and Normalization\n",
    "\n",
    "Normalize features to have zero mean and unit variance for better training stability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf8815c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize features using StandardScaler\n",
    "scaler = StandardScaler()\n",
    "node_features_scaled = scaler.fit_transform(node_features)\n",
    "\n",
    "# Convert to PyTorch tensors\n",
    "x = torch.tensor(node_features_scaled, dtype=torch.float)\n",
    "\n",
    "print(f\"Scaled features shape: {x.shape}\")\n",
    "print(f\"Feature statistics after scaling:\")\n",
    "print(f\"  Mean: {x.mean(dim=0)[:5]}...\")  # Show first 5\n",
    "print(f\"  Std: {x.std(dim=0)[:5]}...\")\n",
    "\n",
    "# Extract labels for each node\n",
    "node_labels = np.zeros(num_nodes, dtype=np.int64)\n",
    "for user_str in unique_users:\n",
    "    user_idx = user_to_idx[user_str]\n",
    "    # Get the label for this user (take the first occurrence)\n",
    "    label = df[df['author_id_str'] == user_str]['label'].iloc[0]\n",
    "    node_labels[user_idx] = label\n",
    "\n",
    "y = torch.tensor(node_labels, dtype=torch.long)\n",
    "\n",
    "print(f\"\\nLabel distribution in graph:\")\n",
    "unique, counts = torch.unique(y, return_counts=True)\n",
    "for label, count in zip(unique, counts):\n",
    "    print(f\"  Label {label}: {count} ({100*count/num_nodes:.2f}%)\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
